I"q<p>AWS Glue is a tool for performing <strong>ETL (extract, transform and load)</strong> services on underlying data lake. It provides structure to unstructured S3 data chunks and acts as a central metadata repository. This means it is able to discover schemas and table definitions of the contents of S3 files (like CSV, JSON and more) and change the structure of these data so that they can be used by other tools and services, like Amazon Redshift, Amazon Athena or Amazon EMR.</p>

<p><strong>AWS Glue Crawler</strong> is one of the main components of Glue. Its job is to scan the data (either according to a schedule or on demand) and automatically infer the schema. While scanning S3 data lakes, it will also populate the so-called <strong>Glue Data Catalog</strong> - a central metadata repository. This repository holds information such as column names or their data types.</p>

<p>Once we select a data source and data target, the Python or Scala code (since Glue runs on Apache Spark) is automatically generated. Its task is to extract data from the source, transform them to match the target schema and load them into the target structure. This code can easily be edited, debugged and tested.</p>

<h3 id="choosing-smart-s3-directory-structure">Choosing Smart S3 Directory Structure</h3>

<p>Glue Crawler will extract partitions according to how the data is organized in S3 buckets. This is crucial when analyzing performance and efficiency of Glue operations. The main question we have to ask ourselves when designing S3 directory structure is how the data is going to be queried later on. Let’s imagine we are going to analyze weather data from 5 different measuring devices. These data will be recorded systematically, let’s say every 15 minutes. If we primarily want to query these data based on a time range, it would be good to set the folders structure as: <span style="font-family:Courier; color:black">year &gt; month &gt; day &gt; hour</span>. This way, when we want to extract all data for a particular day, the queries that search through these buckets will work more efficiently. If on the other hand, we would like first choose data from a particular device and then look for a specific time range, the device should be on the top of the structure: <span style="font-family:Courier; color:black">device &gt; year &gt; month &gt; day &gt; hour</span>.</p>

<h3 id="glue-etl">Glue ETL</h3>

<p>Glue can automatically generate code if we define transformations we want to apply to data. If there are any errors in the ETL jobs, they will be reported to <strong>CloudWatch</strong> and from there an <strong>SNS (Simple Notification Service)</strong> can be enabled to have automatic notifications. Additionally, Glue has some encryption features to keep data secure.</p>

<p>Glue is <strong>serverless</strong>. This means it works according to a pay-as-you-go rule – we are priced based on the actual amount of resources used when running jobs, and not on some fixed amount of bandwidth or pre-purchased units of server space. This is extremely cost-effective.</p>

<h3 id="when-glue-is-not-a-good-choice">When Glue is not a good choice?</h3>

<p>Glue should not be used for streaming data. It is batch oriented by definition and has the minimum interval of 5 minutes. Also, Glue does not support NoSQL databases (which makes perfect sense since they do not have a rigid schema for the Glue Crawler to find).</p>
:ET